对于一般的机器学习任务，只收集一次资料，就可不断的train
但是，对于RL来说，没一个for循环（梯度更新）都要重新收集一次资料，比如（s1，a1）、（s2，a2）、（s3，a3），所以训练很慢，这也叫采样率太慢。比如

on-policy：当前agent网络的参数为θ_i-1，采样等到一组资料（s1，a1）、（s2，a2）、（s3，a3），根据r算loss，更新θ_i-1，变为θ_i，此时θ_i比θ_i-1更好，相当于agent更聪明了，所以要用这个更聪明的agent去收集新的资料（s1’，a1‘）、（s2’，a2‘）、（s3’，a3‘），再根据新的r来算loss，再去更新θ_i              （总之总结一下就是，θ_i-1收集的资料只能训练θ_i-1，即与环境互动的agent与被训练的agent是同一个），每次更新θ就相当于找到了一个新的agent

off-policy：可以想办法用θ_i-1收集的资料训练θ_i，即收集一次资料可以更新很多次参数，提高采样率，经典的方法有PPO，off-policy的重点就是当前需要训练的agent应该知道它与收集资料的agent的差别

我们希望在收集资料时agent的随机性能够大一些，比如在agent的输出的分布上加噪声或者加大交叉熵，使其能够sample到一些概率较低的action。（exploration技巧）