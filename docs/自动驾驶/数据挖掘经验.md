
##  为什么要数据挖掘？

- 人类在某种场景下有个驾驶数据的分布，智驾产品期望有个不同分布，我们期望挖掘的数据能够尽可能从人类数据分布中采样出满足智驾产品期望的分布
    
- 如果人驾数据的refpath质量较低，需要对refpath进行进一步筛选（如压实线、变道过缓等）
    
- 数学表示

$$P\left(人驾\right)=\Sigma P\left(\text{FST}\right) \pi\left(人驾 \mid \text{FST}\right)$$

									挖掘↓

$$P\left(训练\right)=\Sigma Q_{\phi}\left(\text{FST}\right) \pi^\star\left(人驾 \mid \text{FST}\right)$$

									训练↓

$$\min_{\theta}\Sigma_{\text{FST}} Q_{\phi}\left(\text{FST}\right)\mathcal{L}_2\left[\pi_{\theta}\left(产品 \mid \text{FST}\right) \Vert \pi^\star\left(人驾 \mid \text{FST}\right)\right]$$

- $P\left(\text{FST}\right)$：真实世界的FST分布
- $P\left(人驾\right)$、$P\left(训练\right)$人驾数据分布和训练数据分布
- $\pi\left(人驾 \mid \text{FST}\right)$：FST条件下的人类驾驶策略采样，后续省略“FST条件下的”
- $\pi^\star\left(人驾 \mid \text{FST}\right)$：**最优**人类驾驶策略采样
- $\pi_{\theta}\left(产品 \mid \text{FST}\right)$：训练得到的产品定义的驾驶策略，参数为$\theta$
- $Q_{\phi}\left(\text{FST}\right)$：数据配比分布，超参$\phi$可以根据FST重要程度和FST难易程度来调整

## 特征角度

**挖掘脚本中本身自带特征**

- 模型的学习问题可以看成：N种稀疏特征在某些阈值范围内->某一类别的refpath/轨迹
    - 性能表现较差的场景需要从提测、自闭环中去统计和总结，找到这些场景的共性，才能发现模型没学好的特征是什么
    - 特征需要自己在PDV中去评价meta准确性，如果不够好或者缺失，则需要自己开发新meta
- 可以根据特征去对细分FST拆解，方便调配比
- 举例：路口前xx米，有xxx导航信息，有虚线，有/无地面路标->一类左变道的refpath/轨迹

## 关联性角度

- 真实关联性特征
- 虚假关联性特征
    - 非常容易学到，在不同数据之间是不稳定的（可以利用这一特性降低甚至消除影响）
- 需要加数据对冲，让模型区分哪些特征是真值/虚假的

暂时无法在飞书文档外展示此内容
- 举例1：
    - A和决策强关联，B偶尔和A一起出现，B和决策关联性也较弱，训练中容易忽略B，使得单独出现B时容易出错
    - C虽然是一般关联，但是没有关联其他特征，比较纯粹，模型容易学到
    - D和A强关联，训练过程中容易学到D和决策的虚假关联性，需要加数据对冲，例如加只有A没有D、只有D没有A的正向和反向好数据，甚至可以只学有A没有D的数据防止学到D的虚假关联性
- 举例2：
      大曲率弯道+前车+地标->变道的场景，需要加：
    - 大曲率弯道+有前车+无地标->不变道，大曲率弯道+无前车+地标->变道，让模型知道前车不是变道的真实关联特征
    - 可能产生问题：大曲率弯道下不合理变道（实际表现：触发好，抑制差；触发差，抑制好），需要加抑制数据：大曲率弯道+无前车+无地标->不变道，告诉模型变道是因为地标而不是大曲率弯道
    - 新的问题：前车长时间（超过模型输入时间）遮挡地标情况下的gt变道数据要学吗？猜测：学了这类数据更容易学到：大曲率弯道+有前车->不合理超车变道这种虚假关联性，容易让模型混淆，建议不学


## 样本难易角度

**数据越多越好吗？**
- 大部分训练数据，包括大多数clip和clip中的大多数帧，都是对训练帮助不大的数据
- 少量关键的训练数据就能让模型学到好的特征关联性
- 从合训的角度，固定训练轮数的情况下，无效数据越多比例，有效数据被训练到的次数/概率就更低
- 如何验证数据是否有效：
    - 做过拟合实验，如数据配比提高10-20倍，观察评测问题是否全解
        - 没有全解：看到底哪些场景效果差，需要提升数据recall，或者被其他数据（例如自己的脏数据或者别的FST脏数据）抑制，或者学到了虚假关联性特征
        - 几乎全解：调成正常配比，如果指标下降很明显，说明不需要太关注数据recall提升，而是因为无效数据多，需要去除简单场景的无效数据，或者缩小每个clip的ROI范围到关键片段
            
- Model Mining/云端影子模式挖掘：
    - 对比较难挖掘且数据量非常大的FST，可以找到Hard Good数据和脏数据，例如抑制变道类FST
    - 找出对梯度贡献大的好数据，使得在有限训练次数的合训中，该FST充分、有效得到训练


## **数据误差估计**

- 数据收集阶段，采样分布的误差（开车习惯、人脑地图先验等）
- 数据挖掘的分布误差（可能挖到脏数据，挖掘的召回率低使得数据分布改变）
- 模型建模误差（对轨迹的一些假设简单，模态数量有限，BEV特征的实际范围有限，fixed BEV特征的pre-trained模型性能有限）
- 模型拟合误差（欠拟合/过拟合）
- 兜底策略误差（兜底方案使得某些轨迹会被修改、抑制）
- 执行误差（轨迹跟踪误差）
## BadEvents学习

Motivation：有的场景正向数据不好专项挖掘，反向数据反而更好挖掘/更有训练价值，且路测有大量反向的负例数据，可以实现真正的数据闭环。认为BadEvents数据可能能比GoodEvents产生更大梯度，提高数据学习效率
- 模型中decision的意义是所有数据里最像人的才是1，而不是行为正确的是1
- BadEvents只能抑制
TODO：
- BadEvents抑制谁？最像gt的？
## Model Loss

TODO
## 数据训练采样比例
假设模型训练20K
训练Frames总数量：8*8*8*20*1000=1024w
DLP数据配比：100,~~1200~~,400,400,300，外部比例1/3
内部比例：x/(160+x)，x为新增，160为0930Base统计结果
内部配比为1的数据，每20K会被训练到：1024/3/160=2.13w
以train_dlp_lane_change_clip_data为例，内部配比20，600w帧
训练到的比例：2.13*20/600 = 7.1%

## 好数据的定义

- Model Loss&采样理论：能对模型产生较大梯度的关键场景（经常出错的场景），ROI还要限制非常严，不太能产生梯度的无效好数据/非关键帧加入，反而会降低关键数据被采样的概率，真正值得训练的好数据可能只有5%
    - 变道场景ROI Start ：有变道意图（例如方向盘开始转动）或者有与变道相关的特征出现（例如地面出现汇流地标）开始。如果roi_start过于提前，会学到一些没有因果性的数据（没观察到特征，监督数据却要变道），如果roi_start过晚，可能决策时间偏晚
    - 变道场景ROI End：到跨线点。过长会有太多跨线后的无效数据。过短可能在跨线点来回变道？
    - 训练数据是一批触发决策的关键帧，时长可能只有0.5-2秒
- 数据空洞理论：特定场景未加训练数据，可能出现不合理行为
- 脏数据理论：脏数据会对模型产生负面影响，且一个组的好数据可能是另一个组的脏数据；人驾数据中存在许多脏数据
- 场景细分理论：为了找到好数据，应该对场景细分，例如细分成1000个场景，分别加高质量数据训练

## 好模型的定义

- 轨迹/路径的完备性，左中右都有
- 能够建模多模态的、复杂的行为，例如超车、绕障
- 影响模型决策的物体/事件，在模型特征中都能很好的学到，例如自车速度、前车是个慢车、路上地标
- ……
## **RL**

- 数据来源：人驾模式+筛选器
- 数据结构：状态、动作、价值对 （S，A，R）
- reward来源：数据返回后代码解析计算，根据非常简单的一些逻辑（稀疏或者稠密奖励）
- 动作Sampling来源：同一个相似场景，不同人驾的动作作为动作多样性来源
- Imitation Learning的pre-trained模型作为热启动
- 轨迹建模：自回归模型（可以考虑离散化输出）
- Policy网络：原来Imitation Learning出轨迹的网络
- Value网络：额外增加
- Model-based？：TODO

## **数据来源**

- 纯人驾数据（影子模式/筛选器）：off-policy，高方差
- 纯智驾数据：on-policy，含有失败数据
    - 智驾接管数据：on-policy，有人工标注的负例（接管前）和矫正正例（接管后）
    - 智驾issue数据（非接管级别数据）：on-policy，非接管级别只有人工标注的负例